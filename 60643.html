From 1171bc1d55996fb166b522edfa13d897b5d6d65d Mon Sep 17 00:00:00 2001
From: Sebastien Buisson <sbuisson@ddn.com>
Date: Mon, 11 Aug 2025 14:00:49 +0200
Subject: [PATCH 1/1] LU-19157 nodemap: revoke export locks on nodemap change

When a nodemap definition is changed, the locks for the member exports
should be revoked under certain conditions. Fix the current mechanism
in ldlm_revoke_export_locks() as it does nothing because -EOPNOTSUPP
is returned from cfs_hash_for_each_nolock().
Member exports associated with a nodemap should not be revoked after
changes occured on a different nodemap. And for member exports
associated with the nodemap being changed, not all changes require to
revoke locks. We care about clients caching permissions that are no
longer correct, so we are interested in nodemap properties such as
admin, trusted, squash ids, offsets, and id mappings as well.
And we also have to revoke locks for clients being banned.

sanity-sec test_81 is used to verify that the lock revoking process
works fine with banlist, and sanity-sec test_82 is added to exercise
the lock revoking process in case of nodemap changes.
Also fix sanity-sec test_21 to make sure lru locks are flushed after
nodemaps have been modified, and that this is done on all clients.

Signed-off-by: Sebastien Buisson <sbuisson@ddn.com>
Change-Id: Ibca0f280ea70dcd32256c4c6f5252cb6b6b18171
---
 lustre/ldlm/ldlm_lockd.c       |  83 +++++----------
 lustre/ptlrpc/nodemap_member.c | 183 +++++++++++++++++++++++++++++++--
 lustre/tests/sanity-sec.sh     |  63 ++++++++++--
 3 files changed, 254 insertions(+), 75 deletions(-)

diff --git a/lustre/ldlm/ldlm_lockd.c b/lustre/ldlm/ldlm_lockd.c
index d39365ce96..9d1677202d 100644
--- a/lustre/ldlm/ldlm_lockd.c
+++ b/lustre/ldlm/ldlm_lockd.c
@@ -2707,71 +2707,38 @@ static int ldlm_hpreq_handler(struct ptlrpc_request *req)
 	RETURN(0);
 }
 
-static int ldlm_revoke_lock_cb(struct cfs_hash *hs, struct cfs_hash_bd *bd,
-			       struct hlist_node *hnode, void *data)
-
-{
-	struct list_head *rpc_list = data;
-	struct ldlm_lock *lock = cfs_hash_object(hs, hnode);
-
-	lock_res_and_lock(lock);
-
-	if (!ldlm_is_granted(lock)) {
-		unlock_res_and_lock(lock);
-		return 0;
-	}
-
-	LASSERT(lock->l_resource);
-	if (lock->l_resource->lr_type != LDLM_IBITS &&
-	    lock->l_resource->lr_type != LDLM_PLAIN) {
-		unlock_res_and_lock(lock);
-		return 0;
-	}
-
-	if (ldlm_is_ast_sent(lock)) {
-		unlock_res_and_lock(lock);
-		return 0;
-	}
-
-	LASSERT(lock->l_blocking_ast);
-	LASSERT(!lock->l_blocking_lock);
-
-	ldlm_set_ast_sent(lock);
-	if (lock->l_export && lock->l_export->exp_lock_hash) {
-		/*
-		 * NB: it's safe to call cfs_hash_del() even lock isn't
-		 * in exp_lock_hash.
-		 */
-		/*
-		 * In the function below, .hs_keycmp resolves to
-		 * ldlm_export_lock_keycmp()
-		 */
-		cfs_hash_del(lock->l_export->exp_lock_hash,
-			     &lock->l_remote_handle, &lock->l_exp_hash);
-	}
-
-	list_add_tail(&lock->l_rk_ast, rpc_list);
-	ldlm_lock_get(lock);
-
-	unlock_res_and_lock(lock);
-	return 0;
-}
-
 void ldlm_revoke_export_locks(struct obd_export *exp)
 {
-	int rc;
-	LIST_HEAD(rpc_list);
+	struct lu_env *env = lu_env_find();
+	struct lu_env _env;
+	int rc = 0;
 
 	ENTRY;
 
-	cfs_hash_for_each_nolock(exp->exp_lock_hash,
-				 ldlm_revoke_lock_cb, &rpc_list, 0);
-	rc = ldlm_run_ast_work(exp->exp_obd->obd_namespace, &rpc_list,
-			  LDLM_WORK_REVOKE_AST);
+	if (!env) {
+		rc = lu_env_init(&_env, LCT_DT_THREAD);
+		if (rc)
+			RETURN_EXIT;
+		env = &_env;
+		rc = lu_env_add(env);
+		if (rc)
+			GOTO(out_env_fini, rc);
+	}
 
-	if (rc == -ERESTART)
-		ldlm_reprocess_recovery_done(exp->exp_obd->obd_namespace);
+	/* From ldlm_bl_thread_exports:
+	 * If the given export has blocked locks, the next in the list may have
+	 * them too, thus cancel regular locks only if the current export has
+	 * no blocked locks.
+	 */
+	rc = ldlm_export_cancel_blocked_locks(exp);
+	if (rc == 0)
+		ldlm_export_cancel_locks(exp);
 
+	if (env == &_env) {
+		lu_env_remove(env);
+out_env_fini:
+		lu_env_fini(env);
+	}
 	EXIT;
 }
 EXPORT_SYMBOL(ldlm_revoke_export_locks);
diff --git a/lustre/ptlrpc/nodemap_member.c b/lustre/ptlrpc/nodemap_member.c
index 824a544c9d..c4d5941223 100644
--- a/lustre/ptlrpc/nodemap_member.c
+++ b/lustre/ptlrpc/nodemap_member.c
@@ -9,6 +9,7 @@
 #include <linux/module.h>
 #include <lustre_net.h>
 #include <obd_class.h>
+#include <linux/capability.h>
 #include "nodemap_internal.h"
 
 #define HASH_NODEMAP_MEMBER_BKT_BITS 3
@@ -122,21 +123,163 @@ int nm_member_add(struct lu_nodemap *nodemap, struct obd_export *exp)
 }
 
 /*
- * Revokes the locks on an export if it is attached to an MDT and not in
- * recovery. As a performance enhancement, the lock revoking process could
- * revoke only the locks that cover files affected by the nodemap change.
+ * Revokes the locks on an export if it is not in recovery, and attached to
+ * an MDT, or an OST if force_ost is true.
+ * To not break server to server communications, we skip lock revoking for LWP
+ * and loopback connections.
  */
-static void nm_member_exp_revoke(struct obd_export *exp)
+static void nm_member_exp_revoke(struct obd_export *exp, bool force_ost)
 {
 	struct obd_type *type = exp->exp_obd->obd_type;
-	if (strcmp(type->typ_name, LUSTRE_MDT_NAME) != 0)
+
+	if (!force_ost && strcmp(type->typ_name, LUSTRE_MDT_NAME) != 0)
 		return;
 	if (test_bit(OBDF_RECOVERING, exp->exp_obd->obd_flags))
 		return;
+	if (nid_is_lo0(&exp->exp_connection->c_peer.nid) ||
+	    is_lwp_on_ost(exp->exp_client_uuid.uuid) ||
+	    is_lwp_on_mdt(exp->exp_client_uuid.uuid))
+		return;
 
 	ldlm_revoke_export_locks(exp);
 }
 
+/* Cache for nodemap_change_need_update() results.
+ * As comparing nodemap properties can be time consuming, a temporary cache is
+ * created for each nodemap being reclassified. Cache entries contain a
+ * reference to the nodemap being compared with, and the comparison result.
+ */
+struct nm_cmp_cache_entry {
+	struct lu_nodemap *cce_nm;
+	bool		   cce_need_update;
+	struct rhash_head  cce_node;
+};
+
+static void nm_cmp_cache_free(void *ptr, void *arg)
+{
+	struct nm_cmp_cache_entry *entry = ptr;
+
+	OBD_FREE_PTR(entry);
+}
+
+static const struct rhashtable_params nm_cmp_cache_params = {
+	.head_offset = offsetof(struct nm_cmp_cache_entry, cce_node),
+	.key_offset  = offsetof(struct nm_cmp_cache_entry, cce_nm),
+	.key_len     = sizeof(struct lu_nodemap *),
+	.automatic_shrinking = true,
+};
+
+static struct rhashtable nm_cmp_cache;
+static bool use_nm_cmp_cache;
+
+/* Return true if idmaps are identical */
+static bool idmaps_match(struct rb_root *old, struct rb_root *new)
+{
+	struct lu_idmap	*idmapold, *idmapnew;
+	struct rb_node *nold = rb_first(old);
+	struct rb_node *nnew = rb_first(new);
+
+	while (nold && nnew) {
+		idmapold = rb_entry(nold, struct lu_idmap, id_fs_to_client);
+		idmapnew = rb_entry(nnew, struct lu_idmap, id_fs_to_client);
+
+		if (idmapold->id_fs != idmapnew->id_fs ||
+		    idmapold->id_client != idmapnew->id_client)
+			return false;
+
+		nold = rb_next(nold);
+		nnew = rb_next(nnew);
+	}
+
+	if (nold || nnew)
+		return false;
+
+	return true;
+}
+
+/**
+ * nodemap_change_need_update() - Compare old and new nodemap definitions
+ * @old_nodemap: old nodemap
+ * @new_nodemap: new nodemap
+ *
+ * If nodemaps are different, the client must revoke its locks.
+ * Callers should hold the active_config_lock and active_config
+ * nmc_range_tree_lock and nm_member_list_lock and nm_idmap_lock.
+ *
+ * Return:
+ * * %true if nodemap changes require to revoke client locks
+ * * %false otherwise
+ */
+static bool nodemap_change_need_update(struct lu_nodemap *old,
+				       struct lu_nodemap *new)
+{
+	struct nm_cmp_cache_entry *entry;
+	bool res = true;
+
+	if (use_nm_cmp_cache) {
+		struct nm_cmp_cache_entry *found;
+
+		found = rhashtable_lookup_fast(&nm_cmp_cache, &new,
+					       nm_cmp_cache_params);
+		if (found)
+			return found->cce_need_update;
+	}
+
+	/* If old and new nodemap names are different, client was moved to a
+	 * different nodemap. This requires the client to revoke its locks.
+	 */
+	if (strcmp(old->nm_name, new->nm_name))
+		goto out_change;
+
+	/* We do not want clients to cache permissions that are no longer
+	 * correct. So any changes to properties below require to revoke locks.
+	 */
+	if (old->nmf_trust_client_ids != new->nmf_trust_client_ids ||
+	    old->nmf_allow_root_access != new->nmf_allow_root_access ||
+	    old->nmf_deny_unknown != new->nmf_deny_unknown ||
+	    old->nmf_map_mode != new->nmf_map_mode ||
+	    old->nmf_caps_type != new->nmf_caps_type ||
+	    old->nm_squash_uid != new->nm_squash_uid ||
+	    old->nm_squash_gid != new->nm_squash_gid ||
+	    old->nm_squash_projid != new->nm_squash_projid ||
+	    old->nm_offset_start_uid != new->nm_offset_start_uid ||
+	    old->nm_offset_limit_uid != new->nm_offset_limit_uid ||
+	    old->nm_offset_start_gid != new->nm_offset_start_gid ||
+	    old->nm_offset_limit_gid != new->nm_offset_limit_gid ||
+	    old->nm_offset_start_projid != new->nm_offset_start_projid ||
+	    old->nm_offset_limit_projid != new->nm_offset_limit_projid ||
+	    !cap_issubset(old->nm_capabilities, new->nm_capabilities) ||
+	    !cap_issubset(new->nm_capabilities, old->nm_capabilities))
+		goto out_change;
+
+	/* Same for id mappings */
+	if (!idmaps_match(&old->nm_fs_to_client_uidmap,
+			  &new->nm_fs_to_client_uidmap) ||
+	    !idmaps_match(&old->nm_fs_to_client_gidmap,
+			  &new->nm_fs_to_client_gidmap) ||
+	    !idmaps_match(&old->nm_fs_to_client_projidmap,
+			  &new->nm_fs_to_client_projidmap))
+		goto out_change;
+
+	res = false;
+
+out_change:
+	if (!use_nm_cmp_cache)
+		goto out_end;
+
+	/* best effort to create a cache entry, do not fail on error */
+	OBD_ALLOC_PTR(entry);
+	if (entry) {
+		entry->cce_nm = new;
+		entry->cce_need_update = res;
+		if (rhashtable_insert_fast(&nm_cmp_cache, &entry->cce_node,
+					   nm_cmp_cache_params))
+			OBD_FREE_PTR(entry);
+	}
+out_end:
+	return res;
+}
+
 /**
  * nm_member_reclassify_nodemap() - Reclassify members of a nodemap
  * @nodemap: nodemap with members to reclassify
@@ -163,7 +306,7 @@ void nm_member_reclassify_nodemap(struct lu_nodemap *nodemap)
 	list_for_each_entry_safe(exp, tmp, &nodemap->nm_member_list,
 				 exp_target_data.ted_nodemap_member) {
 		struct lnet_nid *nid;
-		bool banned;
+		bool banned, newly_banned;
 
 		/* if no conn assigned to this exp, reconnect will reclassify */
 		spin_lock(&exp->exp_lock);
@@ -175,6 +318,10 @@ void nm_member_reclassify_nodemap(struct lu_nodemap *nodemap)
 		}
 		spin_unlock(&exp->exp_lock);
 
+		if (!use_nm_cmp_cache &&
+		    !rhashtable_init(&nm_cmp_cache, &nm_cmp_cache_params))
+			use_nm_cmp_cache = true;
+
 		/* nodemap_classify_nid requires nmc_range_tree_lock and
 		 * nmc_ban_range_tree_lock
 		 */
@@ -184,14 +331,15 @@ void nm_member_reclassify_nodemap(struct lu_nodemap *nodemap)
 		if (IS_ERR(new_nodemap))
 			continue;
 
-		if (banned) {
+		newly_banned = banned && !exp->exp_banned;
+		if (newly_banned) {
 			LCONSOLE_WARN(
 			       "%s: nodemap %s banning client %s (at %s)\n",
 			       exp->exp_obd->obd_name, new_nodemap->nm_name,
 			       obd_uuid2str(&exp->exp_client_uuid),
 			       obd_export_nid2str(exp));
 			exp->exp_banned = 1;
-		} else if (exp->exp_banned) {
+		} else if (!banned && exp->exp_banned) {
 			LCONSOLE_WARN(
 			       "%s: nodemap %s un-banned client %s (at %s)\n",
 			       exp->exp_obd->obd_name, new_nodemap->nm_name,
@@ -221,12 +369,25 @@ void nm_member_reclassify_nodemap(struct lu_nodemap *nodemap)
 				 &new_nodemap->nm_member_list);
 			mutex_unlock(&new_nodemap->nm_member_list_lock);
 
-			if (nodemap_active)
-				nm_member_exp_revoke(exp);
+			if (nodemap_active) {
+				down_read(&nodemap->nm_idmap_lock);
+				if (newly_banned ||
+				    nodemap_change_need_update(nodemap,
+							       new_nodemap))
+					nm_member_exp_revoke(exp, banned);
+				up_read(&nodemap->nm_idmap_lock);
+			}
 		} else {
 			nodemap_putref(new_nodemap);
 		}
 	}
+
+	if (use_nm_cmp_cache) {
+		rhashtable_free_and_destroy(&nm_cmp_cache,
+					    nm_cmp_cache_free, NULL);
+		use_nm_cmp_cache = false;
+	}
+
 	mutex_unlock(&nodemap->nm_member_list_lock);
 
 	EXIT;
@@ -257,6 +418,6 @@ void nm_member_revoke_locks_always(struct lu_nodemap *nodemap)
 	mutex_lock(&nodemap->nm_member_list_lock);
 	list_for_each_entry_safe(exp, tmp, &nodemap->nm_member_list,
 			    exp_target_data.ted_nodemap_member)
-		nm_member_exp_revoke(exp);
+		nm_member_exp_revoke(exp, false);
 	mutex_unlock(&nodemap->nm_member_list_lock);
 }
diff --git a/lustre/tests/sanity-sec.sh b/lustre/tests/sanity-sec.sh
index a9087b3fe8..bd0da6644a 100755
--- a/lustre/tests/sanity-sec.sh
+++ b/lustre/tests/sanity-sec.sh
@@ -1296,10 +1296,11 @@ fops_test_setup() {
 
 	do_facet mgs $LCTL nodemap_modify --name c0 --property admin --value 1
 	do_facet mgs $LCTL nodemap_modify --name c0 --property trusted --value 1
-
-	wait_nm_sync c0 admin_nodemap
 	wait_nm_sync c0 trusted_nodemap
 
+	do_nodes $(comma_list $clients) $LCTL set_param \
+		ldlm.namespaces.$FSNAME-MDT*.lru_size=clear
+
 	do_node ${clients_arr[0]} rm -rf $DIR/$tdir
 	nm_test_mkdir
 	do_node ${clients_arr[0]} chown $user $DIR/$tdir
@@ -1308,13 +1309,11 @@ fops_test_setup() {
 		--property admin --value $admin
 	do_facet mgs $LCTL nodemap_modify --name c0 \
 		--property trusted --value $trust
+	wait_nm_sync c0 trusted_nodemap
 
 	# flush MDT locks to make sure they are reacquired before test
-	do_node ${clients_arr[0]} $LCTL set_param \
+	do_nodes $(comma_list $clients) $LCTL set_param \
 		ldlm.namespaces.$FSNAME-MDT*.lru_size=clear
-
-	wait_nm_sync c0 admin_nodemap
-	wait_nm_sync c0 trusted_nodemap
 }
 
 # fileset test directory needs to be initialized on a privileged client
@@ -10458,6 +10457,58 @@ test_81b() {
 }
 run_test 81b "banned client does not block other"
 
+test_82() {
+	local client_ip=$(host_nids_address $HOSTNAME $NETTYPE)
+	local client_nid=$(h2nettype $client_ip)
+	local tf=$DIR/$tdir/$tfile
+	local nm1=c0
+	local nm2=c1
+
+	(( $MDS1_VERSION >= $(version_code 2.16.58) )) ||
+		skip "Need MDS version >= 2.16.58 for export lock revoke"
+
+	stack_trap cleanup_local_client_nodemap_with_mounts EXIT
+	stack_trap "cleanup_local_client_nodemap $nm2" EXIT
+
+	# create data before nodemap setup
+	$LFS mkdir -i 0 -c 1 $DIR/$tdir || error "mkdir $DIR/$tdir failed"
+	echo hi > $tf || error "create $tf failed"
+	chmod 600 $tf || error "chmod 600 $tf failed"
+
+	# setup nodemaps
+	setup_local_client_nodemap $nm2 1 1
+	do_facet mgs $LCTL nodemap_del_range \
+		--name $nm2 --range $client_nid ||
+		error "del range $client_nid to $nm2 failed"
+	setup_local_client_nodemap $nm1 1 1
+
+	# access test file
+	cancel_lru_locks
+	cat $tf || error "from $nm1, cat $tf failed"
+
+	# remove admin from nm1
+	do_facet mgs $LCTL nodemap_modify --name $nm1 \
+		--property admin --value 0 ||
+		error "modify admin=0 on $nm1 failed"
+	wait_nm_sync $nm1 admin_nodemap
+
+	# access test file, should now fail
+	cat $tf && error "cat $tf should fail"
+
+	# move client to other nodemap
+	do_facet mgs $LCTL nodemap_del_range \
+		--name $nm1 --range $client_nid ||
+		error "del range $client_nid to $nm1 failed"
+	do_facet mgs $LCTL nodemap_add_range \
+		--name $nm2 --range $client_nid ||
+		error "add range $client_nid to $nm2 failed"
+	wait_nm_sync $nm2 ranges
+
+	# access test file, should now work
+	cat $tf || error "from $nm2, cat $tf failed"
+}
+run_test 82 "export lock revoked after nodemap change"
+
 test_83() {
 	local user=$(getent passwd $RUNAS_ID | cut -d: -f1)
 	local grp=grptest83
