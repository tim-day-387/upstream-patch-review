<html lang="en">
<body>
<pre>
From 4ceb87d0eb56d9353ba8736fbde577cc54baaed6 Mon Sep 17 00:00:00 2001
From: Qian Yingjin <qian@ddn.com>
Date: Tue, 14 Oct 2025 16:33:48 +0800
Subject: [PATCH 1/1] LU-19469 llite: hole-aware read optimization for truncate
 extend

A client can cache hole extent information locally and serve
zero-filled pages for hole regions avoiding long client I/O path,
without network round-trips and bulk data transfer, significantly
improving performance for sparse file read operations while
maintainint data consistency through Lustre's existing DLM
infrastructure.

This patch implements hole-aware read optimization for the holes
generated by extended truncate().

The hole extents are managed using the existing @osc_extent data
structure with the following enhancements:
- hole marker: Add a flag (oe_hole) to distinguish hole extents
  from regular data extents;
- Rbtree integration: Store hole extents in the same osc_object->
  oo_root tree alongside regular extents;
- State management: define specific states for hole extent
  lifecycle (e.g., OES_CACHE with oe_hole=1);
- Lock association (optional): Maintain association between hole
  extents and protecting DLM locks;

The hole extent will be inserted into per-object extent rbtree
when a client executed extended truncate operation and extended
the file beyond current size.
Page read or readahead will try to check the OSC object's extent
tree. If found a matched hole extent, directly zero-fill pages
for the hole regions and mark pages as uptodate.
The hole extent will be removed when detect the write operations
overlapping with the cached hole extent.
Lock blocking AST will also remove conflicting hole extents from
the rbtree of the object.

Add sanity/test_855{a, b} to verify it.

Signed-off-by: Yingjin Qian <qian@ddn.com>
Change-Id: I53ffaded1bc92fb60ceecf6b3acc78e979713953
---
 lustre/include/cl_object.h    |  12 +++
 lustre/include/lustre_osc.h   |  15 ++-
 lustre/llite/file.c           |   2 +-
 lustre/llite/llite_internal.h |   2 +
 lustre/llite/rw.c             |  69 +++++++++++--
 lustre/llite/vvp_object.c     |   2 +-
 lustre/lov/lov_io.c           |  41 ++++++++
 lustre/mdc/mdc_dev.c          |  12 ++-
 lustre/osc/osc_cache.c        | 176 ++++++++++++++++++++++++++++++++--
 lustre/osc/osc_internal.h     |   5 +
 lustre/osc/osc_io.c           |  47 ++++++++-
 lustre/tests/sanity.sh        |  43 +++++++++
 12 files changed, 402 insertions(+), 24 deletions(-)

diff --git a/lustre/include/cl_object.h b/lustre/include/cl_object.h
index 421b6fb858..bd373c1d6a 100644
--- a/lustre/include/cl_object.h
+++ b/lustre/include/cl_object.h
@@ -1405,6 +1405,13 @@ struct cl_read_ahead {
 	/* Callback data for cra_release routine */
 	void		*cra_dlmlock;
 	void		*cra_oio;
+	void		*cra_holext;
+	/*
+	 * Hole extent information: start and end page index.
+	 * cra_hole_end != 0 indicates a hole extent was detected.
+	 */
+	pgoff_t		 cra_hole_start;
+	pgoff_t		 cra_hole_end;
 
 	/*
 	 * Linkage to track all cl_read_aheads for a read-ahead operations,
@@ -1745,6 +1752,11 @@ enum cl_fsync_mode {
 	CL_FSYNC_ALL		= 3,
 	/** start writeback, thus the kernel can reclaim some memory */
 	CL_FSYNC_RECLAIM	= 4,
+	/**
+	 * start writeback, wait for them finished and release hole extents,
+	 * thus the client can prune the object for layout change.
+	 */
+	CL_FSYNC_PRUNE		= 5,
 };
 
 struct cl_io_rw_common {
diff --git a/lustre/include/lustre_osc.h b/lustre/include/lustre_osc.h
index 6cca827899..300e94d56d 100644
--- a/lustre/include/lustre_osc.h
+++ b/lustre/include/lustre_osc.h
@@ -119,6 +119,8 @@ struct osc_io {
 	 * page writeback from happening.
 	 */
 	struct osc_extent *oi_trunc;
+	/** Original file size before truncate, used for hole extent check. */
+	__u64		   oi_orig_size;
 	/** write osc_lock for this IO, used by osc_extent_find(). */
 	struct osc_lock   *oi_write_osclock;
 	struct osc_lock   *oi_read_osclock;
@@ -923,7 +925,18 @@ struct osc_extent {
 	/** this extent consists of pages that are not directly accessible
 	 *  from the CPU
 	 */
-				oe_is_rdma_only:1;
+				oe_is_rdma_only:1,
+	/**
+	 * this extent represents a hole created by truncate extend or
+	 * fallocate punch. No actual pages, just mark a hole range that should
+	 * return zeros during read.
+	 */
+				oe_hole:1,
+	/**
+	 * an ACTIVE hole extent is being written, so when this extent is
+	 * released, it will need to be invalidated.
+	 */
+				oe_hole_writing:1;
 	/** how many grants allocated for this extent.
 	 *  Grant allocated for this extent. There is no grant allocated
 	 *  for reading extents and sync write extents.
diff --git a/lustre/llite/file.c b/lustre/llite/file.c
index 6cceaf2ab9..ec82390b51 100644
--- a/lustre/llite/file.c
+++ b/lustre/llite/file.c
@@ -5243,7 +5243,7 @@ int cl_sync_file_range(struct inode *inode, loff_t start, loff_t end,
 	ENTRY;
 	if (mode != CL_FSYNC_NONE && mode != CL_FSYNC_LOCAL &&
 	    mode != CL_FSYNC_DISCARD && mode != CL_FSYNC_ALL &&
-	    mode != CL_FSYNC_RECLAIM)
+	    mode != CL_FSYNC_RECLAIM && mode != CL_FSYNC_PRUNE)
 		RETURN(-EINVAL);
 
 	env = cl_env_get(&refcheck);
diff --git a/lustre/llite/llite_internal.h b/lustre/llite/llite_internal.h
index d4ebaaf694..0f49c32394 100644
--- a/lustre/llite/llite_internal.h
+++ b/lustre/llite/llite_internal.h
@@ -763,6 +763,8 @@ struct ra_io_arg {
 	unsigned long	ria_reserved;	/* reserved pages for read-ahead */
 	pgoff_t		ria_end_idx_min;/* minimum end to cover current read */
 	bool		ria_eof;	/* reach end of file */
+	bool		ria_rdpg_hole_zeroed;
+	struct cl_page	*ria_rdpg;
 	/* If stride read pattern is detected, ria_stoff is the byte offset
 	 * where stride read is started. Note: for normal read-ahead, the
 	 * value here is meaningless, and also it will not be accessed
diff --git a/lustre/llite/rw.c b/lustre/llite/rw.c
index d7825a5e77..c9eb5ff075 100644
--- a/lustre/llite/rw.c
+++ b/lustre/llite/rw.c
@@ -173,6 +173,9 @@ enum ll_ra_page_hint {
 	WILLNEED /* this page is gurateed to be needed */
 };
 
+#define RA_PAGE_UPTODATE	1
+#define RA_PAGE_HOLE		2
+
 /**
  * ll_read_ahead_page() - Initiates read-ahead of a page with given index.
  *
@@ -181,6 +184,7 @@ enum ll_ra_page_hint {
  * @queue: struct cl_page_list (list of pages (memory) used for read ahead IO)
  * @index: offset withing page to read ahead
  * @hint: see ll_ra_page_hint
+ * @hole: true if this page is a hole
  *
  * Returns:
  * * %0 if page was added into @queue for read ahead or <0 if page was not
@@ -189,7 +193,7 @@ enum ll_ra_page_hint {
  */
 static int ll_read_ahead_page(const struct lu_env *env, struct cl_io *io,
 			      struct cl_page_list *queue, pgoff_t index,
-			      enum ll_ra_page_hint hint)
+			      enum ll_ra_page_hint hint, bool hole)
 {
 	struct cl_object *clob  = io->ci_obj;
 	struct inode     *inode = vvp_object_inode(clob);
@@ -252,22 +256,32 @@ static int ll_read_ahead_page(const struct lu_env *env, struct cl_io *io,
 			cp->cp_ra_used = 0;
 		}
 
-		cl_page_list_add(queue, cp, true);
+		if (hole) {
+			zero_user(vmpage, 0, PAGE_SIZE);
+			SetPageUptodate(vmpage);
+			cp->cp_defer_uptodate = 0;
+			cl_page_unassume(env, io, cp);
+			rc = RA_PAGE_HOLE;
+		} else {
+			cl_page_list_add(queue, cp, true);
+		}
 	} else {
 		/* skip completed pages */
 		cl_page_unassume(env, io, cp);
 		/* This page is already uptodate, returning a positive number
 		 * to tell the callers about this
 		 */
-		rc = 1;
+		rc = RA_PAGE_UPTODATE;
 	}
 
 	cl_page_put(env, cp);
 
 out:
 	if (vmpage != NULL) {
-		if (rc != 0)
+		if (rc != 0) {
 			unlock_page(vmpage);
+			rc = rc == RA_PAGE_HOLE ? 0 : rc;
+		}
 		put_page(vmpage);
 	}
 	if (msg != NULL && hint == MAYNEED) {
@@ -407,6 +421,12 @@ static bool ras_inside_ra_window(pgoff_t idx, struct ra_io_arg *ria)
 	return false;
 }
 
+static inline bool ll_read_in_hole(struct cl_read_ahead *ra, pgoff_t page_idx)
+{
+	return ra->cra_hole_end > 0 && page_idx >= ra->cra_hole_start &&
+	       page_idx <= ra->cra_hole_end;
+}
+
 static unsigned long
 ll_read_ahead_pages(const struct lu_env *env, struct cl_io *io,
 		    struct cl_page_list *queue, struct ll_readahead_state *ras,
@@ -415,6 +435,9 @@ ll_read_ahead_pages(const struct lu_env *env, struct cl_io *io,
 	struct cl_read_ahead *ra = NULL;
 	/* busy page count is per stride */
 	int rc = 0, count = 0, busy_page_count = 0;
+	struct cl_page *rdpg = ria->ria_rdpg;
+	bool rdpg_hole_checked = false;
+	pgoff_t rdpg_idx = rdpg ? cl_page_index(rdpg) : 0;
 	pgoff_t page_idx;
 
 	LASSERT(ria != NULL);
@@ -457,6 +480,20 @@ ll_read_ahead_pages(const struct lu_env *env, struct cl_io *io,
 
 				list_add_tail(&ra->cra_linkage,
 					      &ria->ria_cl_ra_list);
+
+				if (!rdpg_hole_checked && rdpg &&
+				    ll_read_in_hole(ra, rdpg_idx)) {
+					struct page *vmpage;
+
+					CDEBUG(D_READA, "zero hole page %lu@%pK\n",
+					       rdpg_idx, rdpg);
+					rdpg_hole_checked = true;
+					ria->ria_rdpg_hole_zeroed = true;
+					vmpage = cl_page_vmpage(rdpg);
+					zero_user(vmpage, 0, PAGE_SIZE);
+					SetPageUptodate(vmpage);
+				}
+
 				/*
 				 * Only shrink ria_end_idx if the matched
 				 * LDLM lock doesn't cover more.
@@ -492,7 +529,8 @@ ll_read_ahead_pages(const struct lu_env *env, struct cl_io *io,
 
 			/* If the page is inside the read-ahead window */
 			rc = ll_read_ahead_page(env, io, queue, page_idx,
-						MAYNEED);
+						MAYNEED,
+						ll_read_in_hole(ra, page_idx));
 			if (rc < 0 && rc != -EBUSY)
 				break;
 			if (rc == -EBUSY) {
@@ -928,8 +966,9 @@ static int ll_readpages(const struct lu_env *env, struct cl_io *io,
 	}
 
 	for (page_idx = start; page_idx <= end; page_idx++) {
+		/* TODO: Add hole check here. */
 		ret = ll_read_ahead_page(env, io, queue, page_idx,
-					WILLNEED);
+					WILLNEED, false);
 		if (ret < 0)
 			break;
 		else if (ret == 0) /* ret 1 is already uptodate */
@@ -1777,16 +1816,28 @@ int ll_io_read_page(const struct lu_env *env, struct cl_io *io,
 		INIT_LIST_HEAD(&ria->ria_cl_ra_list);
 		if (ras->ras_next_readahead_idx < cl_page_index(page))
 			skip_index = cl_page_index(page);
+		if (!uptodate)
+			ria->ria_rdpg = page;
+
 		rc2 = ll_readahead(env, io, &queue->c2_qin, ria, ras,
 				   uptodate, file, skip_index,
 				   &ra_start_index);
 		/* Keep iotrace clean. Print only on actual page read */
 		CDEBUG(D_READA | (rc2 ? D_IOTRACE : 0),
-		       DFID " %d pages read ahead at %lu, triggered by user read at %lu, stride offset %lld, stride length %lld, stride bytes %lld\n",
+		       DFID " %d pages read ahead at %lu, triggered by user read at %lu, stride offset %lld, stride length %lld, stride bytes %lld hole_zeroed %d\n",
 		       PFID(ll_inode2fid(inode)), rc2, ra_start_index,
 		       cl_page_index(page), ras->ras_stride_offset,
-		       ras->ras_stride_length, ras->ras_stride_bytes);
-
+		       ras->ras_stride_length, ras->ras_stride_bytes,
+		       ria->ria_rdpg_hole_zeroed);
+
+		if (ria->ria_rdpg_hole_zeroed) {
+			LASSERT(ria->ria_rdpg == page);
+			cl_sync_io_note(env, anchor, 0);
+			page->cp_sync_io = NULL;
+			cl_page_list_move(&queue->c2_qout, &queue->c2_qin,
+					  page);
+			cl_page_unassume(env, io, page);
+		}
 	} else if (cl_page_index(page) == io_start_index &&
 		   io_end_index - io_start_index > 0) {
 		rc2 = ll_readpages(env, io, &queue->c2_qin, io_start_index + 1,
diff --git a/lustre/llite/vvp_object.c b/lustre/llite/vvp_object.c
index be69e0b9ac..38ded7aa65 100644
--- a/lustre/llite/vvp_object.c
+++ b/lustre/llite/vvp_object.c
@@ -137,7 +137,7 @@ static int vvp_prune(const struct lu_env *env, struct cl_object *obj)
 	int rc;
 
 	ENTRY;
-	rc = cl_sync_file_range(inode, 0, OBD_OBJECT_EOF, CL_FSYNC_LOCAL, 1,
+	rc = cl_sync_file_range(inode, 0, OBD_OBJECT_EOF, CL_FSYNC_PRUNE, 1,
 				IO_PRIO_NORMAL);
 	if (rc < 0) {
 		CDEBUG(D_VFSTRACE, DFID ": writeback failed: %d\n",
diff --git a/lustre/lov/lov_io.c b/lustre/lov/lov_io.c
index b5dcbac55f..f2ff020a75 100644
--- a/lustre/lov/lov_io.c
+++ b/lustre/lov/lov_io.c
@@ -724,9 +724,13 @@ static void lov_io_sub_inherit(struct lov_io_sub *sub, struct lov_io *lio,
 		}
 		if (cl_io_is_trunc(io)) {
 			loff_t new_size = parent->u.ci_setattr.sa_attr.lvb_size;
+			__u64 start;
 
 			new_size = lov_size_to_stripe(lsm, index, new_size,
 						      stripe);
+			start = lsm->lsm_entries[index]->lsme_extent.e_start;
+			io->u.ci_setattr.sa_falloc_offset =
+				lov_size_to_stripe(lsm, index, start, stripe);
 			io->u.ci_setattr.sa_attr.lvb_size = new_size;
 			io->u.ci_setattr.sa_attr_uid =
 				parent->u.ci_setattr.sa_attr_uid;
@@ -1265,6 +1269,43 @@ static int lov_io_read_ahead(const struct lu_env *env,
 	/* never exceed the end of the stripe */
 	ra->cra_end_idx = min_t(pgoff_t, ra->cra_end_idx,
 				start + pps - start % pps - 1);
+
+	if (ra->cra_hole_end > 0) {
+		pgoff_t hole_start;
+		pgoff_t hole_end;
+
+		LASSERT(ra->cra_hole_end != CL_PAGE_EOF);
+
+		hole_start = ra->cra_hole_start;
+		ra->cra_hole_start = lov_stripe_pgoff(loo->lo_lsm, index,
+						      hole_start, stripe);
+		/* Boundary of current component */
+		hole_start = lov_io_extent(lio, index)->e_start >> PAGE_SHIFT;
+		if (ra->cra_hole_start < hole_start)
+			ra->cra_hole_start = hole_start;
+
+		hole_end = ra->cra_hole_end;
+		ra->cra_hole_end = lov_stripe_pgoff(loo->lo_lsm, index,
+						    hole_end, stripe);
+		/* Boundary of current component */
+		hole_end = lov_io_extent(lio, index)->e_end >> PAGE_SHIFT;
+		if (hole_end != CL_PAGE_EOF && ra->cra_hole_end >= hole_end)
+			ra->cra_hole_end = hole_end - 1;
+
+		/* Never exceed the end of the stripe */
+		ra->cra_hole_end = min_t(pgoff_t, ra->cra_hole_end,
+					 start + pps - start % pps - 1);
+
+		/* Check whether it is a valid hole extent */
+		if (ra->cra_hole_start > ra->cra_hole_end)
+			ra->cra_hole_end = 0;
+
+		CDEBUG(D_READA, DFID " hole_start=%lu, hole_end=%lu\n",
+		       PFID(lu_object_fid(lov2lu(loo))), ra->cra_hole_start,
+		       ra->cra_hole_end);
+
+	}
+
 	RETURN(0);
 }
 
diff --git a/lustre/mdc/mdc_dev.c b/lustre/mdc/mdc_dev.c
index 1817ff879d..b8b8812323 100644
--- a/lustre/mdc/mdc_dev.c
+++ b/lustre/mdc/mdc_dev.c
@@ -1072,6 +1072,15 @@ static int mdc_io_setattr_start(const struct lu_env *env,
 			struct ost_lvb *lvb = &io->u.ci_setattr.sa_attr;
 			enum cl_attr_valid cl_valid = 0;
 
+			/* Save the original object size for hole extent. */
+			if (cl_io_is_trunc(io)) {
+				oio->oi_orig_size = max(attr->cat_size,
+					io->u.ci_setattr.sa_falloc_offset);
+				CDEBUG(D_CACHE, "["DOSTID"] Truncate hole: %llu/%llu->%llu\n",
+				       POSTID(&loi->loi_oi), loi->loi_kms,
+				       oio->oi_orig_size, size);
+			}
+
 			if (ia_avalid & ATTR_SIZE) {
 				attr->cat_size = size;
 				attr->cat_kms = size;
@@ -1199,7 +1208,8 @@ static int mdc_io_fsync_start(const struct lu_env *env,
 	/* a MDC lock always covers whole object, do sync for whole
 	 * possible range despite of supplied start/end values.
 	 */
-	result = osc_cache_writeback_range(env, osc, 0, CL_PAGE_EOF, 0,
+	result = osc_cache_writeback_range(env, osc, 0, CL_PAGE_EOF,
+					   fio->fi_mode == CL_FSYNC_PRUNE,
 					   fio->fi_mode == CL_FSYNC_DISCARD,
 					   fio->fi_prio);
 	if (result > 0) {
diff --git a/lustre/osc/osc_cache.c b/lustre/osc/osc_cache.c
index aff67cf855..118a1add1a 100644
--- a/lustre/osc/osc_cache.c
+++ b/lustre/osc/osc_cache.c
@@ -69,6 +69,8 @@ static inline char *ext_flags(struct osc_extent *ext, char *flags)
 		*buf++ = 'u';
 	if (ext->oe_memalloc)
 		*buf++ = 'm';
+	if (ext->oe_hole)
+		*buf++ = 'o';
 	if (ext->oe_trunc_pending)
 		*buf++ = 't';
 	if (ext->oe_fsync_wait)
@@ -190,7 +192,7 @@ static int osc_extent_sanity_check0(struct osc_extent *ext,
 			GOTO(out, rc = 55);
 		break;
 	case OES_CACHE:
-		if (ext->oe_grants == 0)
+		if (ext->oe_grants == 0 && !ext->oe_hole)
 			GOTO(out, rc = 60);
 		if (ext->oe_fsync_wait && !ext->oe_urgent && !ext->oe_hp)
 			GOTO(out, rc = 65);
@@ -574,7 +576,7 @@ void osc_extent_release(const struct lu_env *env, struct osc_extent *ext,
 
 	LASSERT(atomic_read(&ext->oe_users) > 0);
 	LASSERT(sanity_check(ext) == 0);
-	LASSERT(ext->oe_grants > 0);
+	LASSERT(ext->oe_grants > 0 || ext->oe_hole);
 
 	if (atomic_dec_and_lock(&ext->oe_users, &obj->oo_lock)) {
 		if (ext->oe_trunc_pending) {
@@ -596,6 +598,17 @@ void osc_extent_release(const struct lu_env *env, struct osc_extent *ext,
 			osc_extent_state_set(ext, OES_TRUNC);
 			ext->oe_trunc_pending = 0;
 			osc_object_unlock(obj);
+		} else if (ext->oe_hole) {
+			LASSERT(ext->oe_state == OES_ACTIVE);
+			if (ext->oe_hole_writing) {
+				ext->oe_hole_writing = 0;
+				__osc_extent_remove(ext);
+				osc_object_unlock(obj);
+				osc_extent_put(env, ext);
+			} else {
+				osc_extent_state_set(ext, OES_CACHE);
+				osc_object_unlock(obj);
+			}
 		} else if (ext->oe_state == OES_ACTIVE) {
 			int grant = 0;
 
@@ -734,7 +747,7 @@ restart:
 			break;
 
 		/* if covering by different locks, no chance to match */
-		if (olck->ols_dlmlock != ext->oe_dlmlock) {
+		if (olck->ols_dlmlock != ext->oe_dlmlock && !ext->oe_hole) {
 			EASSERTF(!overlapped(ext, cur), ext,
 				 EXTSTR"\n", EXTPARA(cur));
 
@@ -752,12 +765,13 @@ restart:
 		if (overlapped(ext, cur)) {
 			/* cur is the minimum unit, so overlapping means
 			 * full contain. */
-			EASSERTF((ext->oe_start <= cur->oe_start &&
-				  ext->oe_end >= cur->oe_end),
+			EASSERTF(ergo(!ext->oe_hole,
+				      (ext->oe_start <= cur->oe_start &&
+				       ext->oe_end >= cur->oe_end)),
 				 ext, EXTSTR"\n", EXTPARA(cur));
 
 			if (ext->oe_state > OES_CACHE || ext->oe_hp ||
-			    ext->oe_fsync_wait) {
+			    ext->oe_fsync_wait || ext->oe_hole) {
 				/* for simplicity, we wait for this extent to
 				 * finish before going forward. */
 				conflict = osc_extent_get(ext);
@@ -812,7 +826,28 @@ restart:
 		osc_extent_insert(obj, cur);
 		OSC_EXTENT_DUMP(D_CACHE, cur, "add into tree %lu/%lu.\n",
 				index, descr->cld_end);
+	} else if (conflict->oe_hole) {
+		LASSERT(conflict->oe_nr_pages == 0);
+		LASSERT(list_empty(&conflict->oe_pages));
+
+		if (conflict->oe_state == OES_CACHE) {
+			/* remove the hole extent */
+			OSC_EXTENT_DUMP(D_CACHE, conflict,
+					"remove HOLE %lu/%lu for ["DOSTID"].\n",
+					conflict->oe_start, conflict->oe_end,
+					POSTID(&obj->oo_oinfo->loi_oi));
+			__osc_extent_remove(conflict);
+			osc_extent_put(env, conflict);
+			osc_extent_put(env, conflict);
+			conflict = NULL;
+			osc_object_unlock(obj);
+			goto restart;
+		}
+
+		if (conflict->oe_state == OES_ACTIVE)
+			conflict->oe_hole_writing = 1;
 	}
+
 	osc_object_unlock(obj);
 
 	if (conflict != NULL) {
@@ -1247,6 +1282,89 @@ static void osc_extent_tree_dump0(int mask, struct osc_object *obj,
 	/* osc_object_unlock(obj); */
 }
 
+void osc_extent_hole_search(struct osc_object *obj, pgoff_t start,
+			    struct cl_read_ahead *ra)
+{
+	struct osc_extent *ext;
+
+	osc_object_lock(obj);
+	ext = osc_extent_search(obj, start);
+	while (ext != NULL && ext->oe_start <= ra->cra_end_idx) {
+		if (ext->oe_hole && (ext->oe_state == OES_CACHE ||
+				     ext->oe_state == OES_ACTIVE)) {
+			ra->cra_hole_start = ext->oe_start;
+			ra->cra_hole_end = ext->oe_end;
+			if (ext->oe_state == OES_CACHE)
+				osc_extent_state_set(ext, OES_ACTIVE);
+			ra->cra_holext = osc_extent_hold(ext);
+			CDEBUG(D_READA, "Found HOLE [%lu-%lu] in [%lu-%lu]\n",
+			       ext->oe_start, ext->oe_end, start,
+			       ra->cra_end_idx);
+			break;
+		}
+
+		ext = next_extent(ext);
+	}
+	osc_object_unlock(obj);
+}
+
+#define EXTENT_HOLE_MIN_PAGES   16
+
+/* Create a hole extent to track truncate-extended or punch hole regions. */
+int osc_extent_hole_create(const struct lu_env *env, struct osc_object *obj,
+			   pgoff_t start, pgoff_t end)
+{
+	struct osc_extent *ext;
+	struct osc_extent *tmp;
+
+	ENTRY;
+
+	if (start > end)
+		RETURN(-EINVAL);
+
+	/* Hole extent is too small, ignore it. */
+	if (end - start + 1 < EXTENT_HOLE_MIN_PAGES)
+		RETURN(0);
+
+	ext = osc_extent_alloc(obj);
+	if (ext == NULL)
+		RETURN(-ENOMEM);
+
+	ext->oe_start = start;
+	ext->oe_end = end;
+	ext->oe_max_end = end;
+	ext->oe_state = OES_CACHE;
+	ext->oe_hole = 1;
+	ext->oe_nr_pages = 0;	/* No autual pages in hole */
+	ext->oe_grants = 0;	/* No grants needed */
+
+	osc_object_lock(obj);
+	tmp = osc_extent_search(obj, start);
+	while (tmp != NULL) {
+		if (tmp->oe_start > end)
+			break;
+
+		/*
+		 * It should not overlap with any other extents as the hole
+		 * extent is inserted by truncate or punch hole.
+		 */
+		EASSERTF(!overlapped(tmp, ext), tmp, "Hole extent "EXTSTR"\n",
+			 EXTPARA(ext));
+
+		/* TODO: Try to merge the previous and next hole extents. */
+		tmp = next_extent(tmp);
+	}
+
+	/* Insert the hole extent into the tree. */
+	osc_extent_insert(obj, ext);
+	osc_object_unlock(obj);
+
+	OSC_EXTENT_DUMP(D_CACHE, ext,
+			"add HOLE extent %lu/%lu for ["DOSTID"].\n",
+			start, end, POSTID(&obj->oo_oinfo->loi_oi));
+	RETURN(0);
+}
+
 /* ------------------ osc extent end ------------------ */
 
 static inline int osc_is_ready(struct osc_object *osc)
@@ -2550,6 +2668,7 @@ int osc_flush_async_page(const struct lu_env *env, struct cl_io *io,
 		LASSERTF(0, "page index %lu is NOT covered.\n", index);
 	}
 
+	LASSERT(!ext->oe_hole);
 	switch (ext->oe_state) {
 	case OES_RPC:
 	case OES_LOCK_DONE:
@@ -3148,6 +3267,7 @@ int osc_cache_writeback_range(const struct lu_env *env, struct osc_object *obj,
 {
 	struct osc_extent *ext;
 	LIST_HEAD(discard_list);
+	LIST_HEAD(holes_list);
 	bool active_ext_check = false;
 	bool unplug = false;
 	int result = 0;
@@ -3165,11 +3285,26 @@ repeat:
 		if (ext->oe_start > end)
 			break;
 
+		if (ext->oe_hole)
+			OSC_EXTENT_DUMP(D_CACHE, ext, "It is HOLE.\n");
 		ext->oe_fsync_wait = 1;
 		switch (ext->oe_state) {
 		case OES_CACHE:
 			result += ext->oe_nr_pages;
-			if (!discard) {
+			/* It is from lock blocking AST or hole punch. */
+			if (ext->oe_hole) {
+				LASSERT(ext->oe_nr_pages == 0);
+				ext->oe_fsync_wait = 0;
+				if (!hp && !discard)
+					break;
+				/*
+				 * Set OES_RPC flag as if it has already
+				 * been sent.
+				 */
+				osc_extent_state_set(ext, OES_RPC);
+				ext->oe_owner = current;
+				list_move_tail(&ext->oe_link, &holes_list);
+			} else if (!discard) {
 				struct list_head *list = NULL;
 
 				if (ext->oe_hp) {
@@ -3225,6 +3360,12 @@ repeat:
 			 * flushed since it may be blocked at awaiting more
 			 * grants. We do this for the correctness of fsync. */
 			LASSERT(hp == 0 && discard == 0);
+
+			if (ext->oe_hole) {
+				ext->oe_fsync_wait = 0;
+				break;
+			}
+
 			ext->oe_urgent = 1;
 
 			if (active_ext_check) {
@@ -3248,6 +3389,22 @@ repeat:
 	}
 	osc_object_unlock(obj);
 
+	LASSERT(ergo(!(hp || discard), list_empty(&holes_list)));
+	if (!list_empty(&holes_list)) {
+		struct osc_extent *tmp;
+
+		list_for_each_entry_safe(ext, tmp, &holes_list, oe_link) {
+			list_del_init(&ext->oe_link);
+			EASSERT(ext->oe_state == OES_RPC && ext->oe_hole &&
+				list_empty(&ext->oe_pages) &&
+				ext->oe_nr_pages == 0, ext);
+
+			OSC_EXTENT_DUMP(D_CACHE, ext, "remove HOLE.\n");
+			/* Finish the extent as if a RPC was sent. */
+			osc_extent_finish(env, ext, 0, 0);
+		}
+	}
+
 	LASSERT(ergo(!discard, list_empty(&discard_list)));
 	if (!list_empty(&discard_list)) {
 		struct osc_extent *tmp;
@@ -3283,8 +3440,9 @@ repeat:
 			result = rc;
 	}
 
-	OSC_IO_DEBUG(obj, "pageout [%lu, %lu] npages %lu: rc=%d.\n",
-		     start, end, obj->oo_npages, result);
+	OSC_IO_DEBUG(obj, "["DOSTID"] pageout [%lu, %lu] npages %lu: rc=%d.\n",
+		     POSTID(&obj->oo_oinfo->loi_oi), start, end,
+		     obj->oo_npages, result);
 
 	/*
 	 * Try to flush the active I/O extents of the object.
diff --git a/lustre/osc/osc_internal.h b/lustre/osc/osc_internal.h
index 3778394449..588e717d02 100644
--- a/lustre/osc/osc_internal.h
+++ b/lustre/osc/osc_internal.h
@@ -26,6 +26,11 @@ int osc_shrink_grant_to_target(struct client_obd *cli, __u64 target_bytes);
 void osc_schedule_grant_work(void);
 void osc_update_next_shrink(struct client_obd *cli);
 void lru_queue_work(struct work_struct *work);
+int osc_extent_hole_create(const struct lu_env *env,
+			   struct osc_object *obj, pgoff_t start,
+			   pgoff_t end);
+void osc_extent_hole_search(struct osc_object *obj, pgoff_t start,
+			    struct cl_read_ahead *ra);
 int osc_extent_finish(const struct lu_env *env, struct osc_extent *ext,
 		      int sent, int rc);
 void osc_extent_release(const struct lu_env *env, struct osc_extent *ext,
diff --git a/lustre/osc/osc_io.c b/lustre/osc/osc_io.c
index 68fb12dbc5..c62f94edff 100644
--- a/lustre/osc/osc_io.c
+++ b/lustre/osc/osc_io.c
@@ -42,10 +42,16 @@ static void osc_io_fini(const struct lu_env *env, const struct cl_io_slice *io)
 
 void osc_read_ahead_release(const struct lu_env *env, struct cl_read_ahead *ra)
 {
+	struct osc_extent *holext = (struct osc_extent *)ra->cra_holext;
 	struct ldlm_lock *dlmlock = ra->cra_dlmlock;
 	struct osc_io *oio = ra->cra_oio;
 	struct lustre_handle lockh;
 
+	if (holext != NULL) {
+		osc_extent_release(env, holext, IO_PRIO_NORMAL);
+		ra->cra_holext = NULL;
+	}
+
 	oio->oi_is_readahead = 0;
 	ldlm_lock2handle(dlmlock, &lockh);
 	ldlm_lock_decref(&lockh, LCK_PR);
@@ -89,6 +95,12 @@ static int osc_io_read_ahead(const struct lu_env *env,
 		ra->cra_end_idx = min_t(pgoff_t,
 					ra->cra_end_idx,
 					(oinfo->loi_kms - 1) >> PAGE_SHIFT);
+
+		ra->cra_hole_end = 0;
+		ra->cra_holext = NULL;
+		/* Check if readahead range overlaps with hole extents. */
+		osc_extent_hole_search(osc, start, ra);
+
 		result = 0;
 	}
 
@@ -709,6 +721,15 @@ static int osc_io_setattr_start(const struct lu_env *env,
 			struct ost_lvb *lvb = &io->u.ci_setattr.sa_attr;
 			enum cl_attr_valid cl_valid = 0;
 
+			/* Save the original object size for hole extent. */
+			if (cl_io_is_trunc(io)) {
+				oio->oi_orig_size = max(attr->cat_size,
+					io->u.ci_setattr.sa_falloc_offset);
+				CDEBUG(D_CACHE, "["DOSTID"] Truncate hole: %llu/%llu->%llu\n",
+				       POSTID(&loi->loi_oi), loi->loi_kms,
+				       oio->oi_orig_size, size);
+			}
+
 			if (ia_avalid & ATTR_SIZE) {
 				attr->cat_size = size;
 				attr->cat_kms = size;
@@ -848,6 +869,27 @@ void osc_io_setattr_end(const struct lu_env *env,
 		osc_trunc_check(env, io, oio, size);
 		osc_cache_truncate_end(env, oio->oi_trunc);
 		oio->oi_trunc = NULL;
+
+		CDEBUG(D_CACHE, "HOLE check [%llu-%llu] for ["DOSTID"]\n",
+		       oio->oi_orig_size, size,
+		       POSTID(&cl2osc(obj)->oo_oinfo->loi_oi));
+		/* Create hole extent if truncate extended the file object. */
+		if (result == 0 && size > oio->oi_orig_size) {
+			pgoff_t hole_start;
+			pgoff_t hole_end;
+			int rc;
+
+			hole_start = (oio->oi_orig_size + PAGE_SIZE - 1) >>
+				     PAGE_SHIFT;
+			hole_end = (size - 1) >> PAGE_SHIFT;
+
+			rc = osc_extent_hole_create(env, cl2osc(obj),
+						    hole_start, hole_end);
+			if (rc < 0)
+				CDEBUG(D_INFO, "Failed to create hole extent [%lu-%lu]/[%llu-%llu]: rc=%d\n",
+				       hole_start, hole_end, oio->oi_orig_size,
+				       size, rc);
+		}
 	}
 
 	if (cl_io_is_fallocate(io)) {
@@ -1095,7 +1137,8 @@ static int osc_io_fsync_start(const struct lu_env *env,
 	if (fio->fi_end == OBD_OBJECT_EOF)
 		end = CL_PAGE_EOF;
 
-	result = osc_cache_writeback_range(env, osc, start, end, 0,
+	result = osc_cache_writeback_range(env, osc, start, end,
+					   fio->fi_mode == CL_FSYNC_PRUNE,
 					   fio->fi_mode == CL_FSYNC_DISCARD,
 					   fio->fi_prio);
 	if (result < 0 && fio->fi_mode == CL_FSYNC_DISCARD) {
@@ -1149,7 +1192,7 @@ void osc_io_fsync_end(const struct lu_env *env,
 	pgoff_t end   = fio->fi_end >> PAGE_SHIFT;
 	int result = 0;
 
-	if (fio->fi_mode == CL_FSYNC_LOCAL) {
+	if (fio->fi_mode == CL_FSYNC_LOCAL || fio->fi_mode == CL_FSYNC_PRUNE) {
 		result = osc_cache_wait_range(env, cl2osc(obj), start, end);
 	} else if (cbargs->opc_rpc_sent && (fio->fi_mode == CL_FSYNC_ALL ||
 					    fio->fi_mode == CL_FSYNC_RECLAIM)) {
diff --git a/lustre/tests/sanity.sh b/lustre/tests/sanity.sh
index a6e1b0ff52..ac1f946c72 100755
--- a/lustre/tests/sanity.sh
+++ b/lustre/tests/sanity.sh
@@ -35645,6 +35645,49 @@ test_854() {
 		error "max_cached_mb changed from 75% of RAM"
 }
 
+test_855a() {
+	local file=$DIR/$tfile
+	local chunk_size=$(( 256 * 1024 ))
+	local repeat=1000
+
+	$LCTL set_param osc.*.stats=clear
+
+	local i
+	local newsz
+	local offset
+	local cmd
+
+	cmd="oO_CREAT:O_RDWR:"
+	for (( i = 1; i <= repeat; i++ )); do
+		newsz=$(( i * chunk_size ))
+		offset=$(((i - 1) * chunk_size))
+		#cmd+="T${newsz}z${offset}r${chunk_size}"
+		cmd+="T${newsz}z${offset}r${chunk_size}z${offset}w${chunk_size}"
+	done
+	cmd+="c"
+
+	echo "CMD: $MULTIOP $file $cmd"
+	time $MULTIOP $file $cmd || error "failed to Truncate/Read/Write $file"
+	sync; sync; sync;
+	$LCTL get_param osc.*.stats
+}
+run_test 855a "Verify that read holes generated by truncate works as expected"
+
+test_855b() {
+	local file=$DIR/$tfile
+
+	$LFS setstripe -E 1M -L mdt -E eof -c2 $file ||
+		error "failed to create PFL file $file"
+	lctl set_param subsystem_debug=llite+lov+osc+mdc
+	lctl set_param debug=trace+vfstrace+info+inode+reada+iotrace+cache
+	lctl clear
+	dd if=/dev/urandom of=$file bs=256K count=1 seek=1 ||
+		error "256K->512K dd failed"
+	dd if=/dev/urandom of=$file bs=1M count=1 seek=2 ||
+		error "2M->3M dd failed"
+}
+run_test 855b "Client-side hole caching for PFL file with DoM component"
+
 #
 # tests that do cleanup/setup should be run at the end
 #

</pre>
</body>
</html>
